tokenizer = vectorizer.build_tokenizer()
tokenized_sentence = tokenizer(preprocessed_sentence)
print(tokenized_sentence)
