{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining of BBC News Data\n",
    "\n",
    "## Part 2: Bag-of-Words Text Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analyzer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"C'est l'été au Brésil!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer().build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(strip_accents=\"unicode\", ngram_range=(2, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(strip_accents=\"unicode\", ngram_range=(1, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(ngram_range=(2, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer = Preprocessor + Tokenizer (+ Token Filtering) (+ n-grams extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents=\"unicode\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"C'est l'été au Brésil!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Type `vectorizer.build_<TAB>` to see the list of methods of the vectorizer object;\n",
    "\n",
    "- Use the vectorizer to build a preprocessor object and apply it to the test sentence: which transformations do you notice?\n",
    "\n",
    "- Use the vectorizer to build a tokenizer object and apply it to the preprocessed test sentence from the previous step;\n",
    "\n",
    "- Compare the results of the previous two steps with the output of the analyzer applied to the original test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/build_preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/build_tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of a Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "bbc_folder_path = Path(\"bbc\")\n",
    "text_filepaths = sorted(bbc_folder_path.glob(\"*/*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of decoding the text manually as we did before, we will path the filenames directly to the vectorizer and let it decode using the encoding of our choice and ignore decoding errors (as we did previously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = CountVectorizer(encoding=\"utf-8\", input=\"filename\",\n",
    "                             decode_error=\"ignore\",\n",
    "                             min_df=5, max_df=0.8)\n",
    "\n",
    "vectorizer.fit(text_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items = sorted(vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorizer.transform(text_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc = vectorized_docs[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: why does the vectorizer return a sparse matrix instead of a regular (dense) array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the indices of the non-zero values in the vectorized first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dimension corresponds to a word in the vocabulary. We can retrieve the \"feature name\" for a specific dimension number as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[5360]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to inverse a full vectorized document at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(first_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Print the text of the original first document. And compare to the \"inversed transformed\" document (can you find the same words?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "\n",
    "- Why do you thing the result of the text vectorization procedure is called \"Bag-of-Words\" representation?\n",
    "\n",
    "- What as the main limitation of the \"Bag-of-Words\" representation?\n",
    "\n",
    "- Can you come up with a pair of sentence with completely different meanings but that would share the same Bag of Words vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Normalization\n",
    "\n",
    "- TF-IDF stands for **\"Term Frequency\" - \"Inverse Document Frequency\"**.\n",
    "\n",
    "- **Term Frequency** is the number of times a term or token appears in a given document;\n",
    "\n",
    "- **Document Frequency** is the number of times a term or token appears across all the documents of the corpus.\n",
    "\n",
    "\n",
    "**Note**: depending on the context, frequencies can either be:\n",
    "\n",
    "- **absolute** values (**integer counts**) or\n",
    "- **relative** values (**floating point numbers between 0 and 1** for **ratio** between two quantities).\n",
    "\n",
    "\n",
    "TF-IDF is a normalization procedures that transforms integer term frequency counts into a reweights term frequencies such that very frequent words (such as \"the\", \"a\", \"and\", \"it\"... in English) have lower importance than rarer words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_docs = tfidf_transformer.fit_transform(vectorized_docs)\n",
    "tfidf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tfidf_doc = tfidf_docs[0, :]\n",
    "first_tfidf_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "    \n",
    "Using the `vectorizer.get_feature_names()` vector and the `tfidf_transformer.idf_`, compute the list of the top 10 least and most informative words in the BBC corpus.\n",
    "\n",
    "Hint: you can put the two vectors in a `pandas.DataFrame` and sort by IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.7",
   "language": "python",
   "name": "python-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
